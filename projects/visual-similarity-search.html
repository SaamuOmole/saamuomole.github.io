<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Visual Similarity Search - Samuel Omole</title>
    <link rel="stylesheet" href="../style.css" />
  </head>
  <body>
    <a href="../index.html">‚Üê Back to home</a>
    <h1>Visual Similarity Search with Deep Metric Learning</h1>
    <p>
      <strong>Focus</strong>: Computer vision and metric learning for visual
      similarity search/image retrieval
    </p>
    <p>
      This project focuses on building an instance-level visual similarity
      search system using deep metric learning. Starting from a pretrained
      ConvNeXt backbone, I fine-tuned an embedding model on the Stanford Online
      Products dataset using supervised contrastive learning and PK sampling to
      learn a discriminative embedding space suitable for image retrieval rather
      than classification.
    </p>
    <p>
      I evaluated the system using Recall@K and demonstrated a substantial
      improvement over baseline pretrained embeddings, confirming that
      task-specific metric learning significantly improves retrieval quality. To
      go beyond benchmark metrics, I performed visual error analysis and tested
      robustness under domain shift by querying the model with real-world phone
      photos against a catalog of product images, revealing strengths and
      failure modes in realistic conditions.
    </p>
    <p>
      The project covers the full applied machine learning workflow, from
      dataset understanding and model training to FAISS-based nearest-neighbor
      search, quantitative evaluation, and qualitative analysis, and reflects
      how visual similarity systems are built and assessed in real-world
      settings.
    </p>

    <h2>What I did</h2>
    <ul>
      <li>
        Built and fine-tuned a ConvNeXt-based embedding model for instance-level
        image retrieval using supervised contrastive learning.
      </li>
      <li>
        Achieved significant Recall@K improvements on the Stanford Online
        Products benchmark with the fine-tuned model relative to the pretrained
        model
      </li>
      <li>
        Validated robustness under domain shift with real-world query images.
      </li>
    </ul>

    <h2>Outputs</h2>
    <li>
      <strong>Fine-tuned visual embedding model</strong>: ConvNeXt-based metric
      learning model trained with supervised contrastive loss, producing 256-D
      embeddings optimised for instance-level retrieval.
    </li>
    <li>
      <strong>Quantitative retrieval evaluation</strong>: Measured Recall@1 /
      Recall@5 / Recall@10 on the Stanford Online Products test set,
      demonstrating clear improvements over pretrained baseline embeddings.
    </li>
    <li>
      <strong>FAISS similarity search index</strong>: Built and evaluated a fast
      nearest-neighbour retrieval pipeline using FAISS with cosine similarity
      for large-scale image search.
    </li>
    <li>
      <strong>Embedding datasets & metadata</strong>: Generated reusable
      embedding artifacts and structured metadata for both baseline and
      fine-tuned models.
    </li>
    <li>
      <strong>Visual error analysis artifacts</strong>: Produced qualitative
      success and failure retrieval examples, enabling diagnosis of viewpoint,
      background, and ambiguity-driven errors.
    </li>
    <li>
      <strong>Domain-shift robustness demo</strong>: Tested the trained model
      using real-world phone photos queried against a benchmark catalog to
      assess behaviour beyond curated datasets.
    </li>
    <li>
      <strong>Reproducible training & evaluation pipeline</strong>: End-to-end
      scripts covering training, embedding extraction, retrieval evaluation, and
      visualisation.
    </li>

    <p>
      <img
        class="figure"
        src="../assets/images/image_retrieval.png"
        alt="Image Retrieval"
        loading="lazy"
      />
      <br />
      <em style="display: block; text-align: center"
        >Successful query (class_id=18968)</em
      >
    </p>

    <p>
      <img
        class="figure"
        src="../assets/images/domain_shift_lamp2.png"
        alt="Domain Shift"
        loading="lazy"
      />
      <br />
      <em style="display: block; text-align: center">Query of my lamp</em>
    </p>

    <p>
      The project is available and fully documented in this repository:
      <a href="https://github.com/SaamuOmole/visual_similarity_sop"
        >Code (GitHub)</a
      >
    </p>
  </body>
</html>
